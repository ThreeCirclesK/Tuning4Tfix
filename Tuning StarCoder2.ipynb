{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5896071c-3d87-49b8-af0f-f1e468be3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast  # For mixed precision\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, PeftConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    "    BatchEncoding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Any, DefaultDict, List, Dict\n",
    "import os, time, socket, argparse\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For loading Tfix dataset \n",
    "from prepare_data import create_data,extract_warning_types\n",
    "from data_reader import GetDataAsPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839cf4d9-c68e-4d8a-955a-993dabed4848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b84e6e-463d-403c-99f6-f1201a315d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(datadir='APRDataset/TFix', model_id='starcoder2-3b', basemodeldir='/media/zero/ssd2/LLMS/starcoder2-3b', modelsavedir='APRModels/StarCoder2-3B_Tfix', output_dir='StarCoder2-3B_TFix', checkpointdir='', attention_dropout=0.1, max_steps=5000, micro_batch_size=8, seed=48, max_seq_length=512, gradient_accumulation_steps=1, weight_decay=0.01, fp16=True, learning_rate=0.0002, lr_scheduler_type='cosine', warmup_steps=500, push_to_hub=False)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--datadir\", type=str, default=\"APRDataset/TFix\")\n",
    "parser.add_argument(\"--model_id\", type=str, default=\"starcoder2-3b\")\n",
    "parser.add_argument(\"--basemodeldir\", type=str, default=f\"/LLMS/starcoder2-3b\") ## change to your directory\n",
    "parser.add_argument(\"--modelsavedir\", type=str, default=f\"APRModels/StarCoder2-3B_Tfix\")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"StarCoder2-3B_TFix\")\n",
    "parser.add_argument(\"--checkpointdir\", type=str, default=\"\")\n",
    "\n",
    "parser.add_argument(\"--attention_dropout\", type=float, default=0.1)\n",
    "parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "parser.add_argument(\"--micro_batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--seed\", type=int, default=48)\n",
    "parser.add_argument(\"--max_seq_length\", type=int, default=512)\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "parser.add_argument(\"--fp16\", type=bool, default=True)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "parser.add_argument(\"--push_to_hub\", type=bool, default=False)\n",
    "\n",
    "# Parse the arguments with the updated defaults\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "set_seed(args.seed)\n",
    "modelname = args.basemodeldir.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c16b1f-9f2c-43e0-ba1f-ed4c9015ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_eos(data):\n",
    "    data = [x.replace(\"</s>\",tokenizer.eos_token) for x in data]\n",
    "    return data\n",
    "def delete_eos(data):\n",
    "    data = [x.replace(\"</s>\",\"\") for x in data]\n",
    "    return data\n",
    "\n",
    "# For batch running\n",
    "class CausalBugFixDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings: BatchEncoding, targets: BatchEncoding, idxs):\n",
    "        self.encodings = encodings\n",
    "        self.target_encodings = targets\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        item = {key: val[index] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.target_encodings[\"input_ids\"][index]\n",
    "        item[\"idx\"] = self.idxs[index]\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "  \n",
    "def create_dataset(\n",
    "    idxs: List[int],\n",
    "    inputs: List[str],\n",
    "    tokenizer,\n",
    ") -> CausalBugFixDataset:\n",
    "    # Based on Transformer version: padding='max_length' or padding=True\n",
    "    input_encodings = tokenizer(\n",
    "        inputs, truncation=True, padding='longest', return_tensors='pt', max_length=512\n",
    "    ) \n",
    "    label_encodings = tokenizer(\n",
    "        inputs, truncation=True, padding='longest', return_tensors='pt', max_length=512\n",
    "    ) \n",
    "    label_encodings[\"input_ids\"][label_encodings[\"input_ids\"] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "\n",
    "    dataset = CausalBugFixDataset(input_encodings, label_encodings, idxs)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f8f51-53b1-4102-a983-9c456fd9dba1",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c868a8aa-d275-46c4-b5e9-37cf73bb8d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starcoder2-3b loaded in 2.594926595687866 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Starcoder2ForCausalLM(\n",
       "  (model): Starcoder2Model(\n",
       "    (embed_tokens): Embedding(49154, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x Starcoder2DecoderLayer(\n",
       "        (self_attn): Starcoder2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=True)\n",
       "          (rotary_emb): Starcoder2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Starcoder2MLP(\n",
       "          (c_fc): Linear4bit(in_features=3072, out_features=12288, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=12288, out_features=3072, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=49154, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Setting for Quantization & LoRA \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "########################\n",
    "\n",
    "start = time.time()\n",
    "model =AutoModelForCausalLM.from_pretrained(args.basemodeldir,quantization_config=bnb_config,\n",
    "                                            device_map={\"\": PartialState().process_index},\n",
    "                                            attention_dropout=args.attention_dropout,\n",
    "                                            )\n",
    "end = time.time()\n",
    "print(f\"{modelname} loaded in\", end- start,\"sec\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.basemodeldir)\n",
    "  \n",
    "if args.checkpointdir!='':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_dir)\n",
    "    model = PeftModel.from_pretrained(model, \n",
    "    checkpoint_dir,\n",
    "     is_trainable=False \n",
    "    )\n",
    "\n",
    "special_tokens_dict = {'pad_token': '<pad>', 'sep_token': '<sep>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resizing the token embeddings to match the tokenizer\n",
    "\n",
    "# Ensure the tokenizer uses left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Typically, EOS is used as the pad token in GPT models\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6e7f7-8fef-4188-9287-3405ec677937",
   "metadata": {},
   "source": [
    "# Load TFix Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b05001e-3be0-44f9-8377-1f1c31efff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of warning types: 52\n",
      "train size: 1039\n",
      "val size: 116\n",
      "test size: 129\n",
      "\n",
      "####### Train data sample\n",
      "fix no-constant-condition Unexpected constant condition. \t\tif (err.code = 'ECONNRESET') {\n",
      ":\n",
      "\tonError(err) {\n",
      "\t\tif (err.code = 'ECONNRESET') {\n",
      "\t\t\tif (!this.retryRegistration) { \n",
      " \n",
      "Fixed: <sep>\n",
      "\tonError(err) {\n",
      "\t\tif (err.code === 'ECONNRESET') {\n",
      "\t\t\tif (!this.retryRegistration) { \n",
      " <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "#  Load Dataset \n",
    "###############\n",
    "data = GetDataAsPython(\"TFixData/data_autofix_tracking_repo_specific_final.json\")\n",
    "data_eslint = GetDataAsPython(\"TFixData/data_autofix_tracking_eslint_final.json\")\n",
    "data += data_eslint\n",
    "all_warning_types = extract_warning_types(data)\n",
    "print(\"# of warning types:\", len(all_warning_types))\n",
    "\n",
    "(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    val_inputs,\n",
    "    val_labels,\n",
    "    test_inputs,\n",
    "    test_labels,\n",
    "    train_info,\n",
    "    val_info,\n",
    "    test_info,\n",
    ") = create_data(data, ['no-constant-condition'], include_warning=True, model_name='')\n",
    "\n",
    "inputs, labels, types, infos = [],[],[],[]\n",
    "for warning_type in all_warning_types:\n",
    "    inputs+=test_inputs[warning_type]\n",
    "    labels+=test_labels[warning_type]\n",
    "    types+=[warning_type]*len(test_labels[warning_type])\n",
    "    infos+=test_info[warning_type]\n",
    "test_data = dict()\n",
    "test_data['buggy']=delete_eos(inputs)\n",
    "test_data['fixed']=change_eos(labels) \n",
    "test_data['info']=infos\n",
    "\n",
    "inputs = delete_eos(val_inputs)\n",
    "labels = change_eos(val_labels)\n",
    "infos = val_info\n",
    "types = [x.linter_report.rule_id for x in val_info]\n",
    "val_data = dict()\n",
    "val_data['buggy']=pa.array(inputs)\n",
    "val_data['fixed']=pa.array(labels)\n",
    "val_data['info']=infos\n",
    "\n",
    "train_inputs = delete_eos(train_inputs)\n",
    "train_labels = change_eos(train_labels)\n",
    "\n",
    "val_inputs = delete_eos(val_inputs)\n",
    "val_labels = change_eos(val_labels)\n",
    "\n",
    "\n",
    "# Change Input format - follow TFix paper\n",
    "train_inputs = [f\"{i}\\nFixed: <sep>\\n{o}\" for i,o in zip(train_inputs, train_labels)]\n",
    "val_inputs = [f\"{i}\\nFixed: <sep>\\n{o}\" for i,o in zip(val_inputs, val_labels)]\n",
    "print(\"\\n####### Train data sample\")\n",
    "print(train_inputs[0])\n",
    "test_inputs = [f\"{i}\\nFixed: \" for i in test_data['buggy']]\n",
    "tidxs = np.arange(len(test_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4dc011-89e3-4d04-8d20-6193c3fd044a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f94a99-526e-445b-90e9-1b80f246fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxx\u001b[0m (\u001b[33mxx\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250129_162452-6pui1y7v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=' ' target=\"_blank\">train-starcoder2-3b-tfix</a></strong> to <a href='' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='' target=\"_blank\"> </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='' target=\"_blank\"> </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/5000 17:40 < 2:02:56, 0.59 it/s, Epoch 4/39]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.462400</td>\n",
       "      <td>3.587370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.555200</td>\n",
       "      <td>3.573278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.761700</td>\n",
       "      <td>3.518967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.258700</td>\n",
       "      <td>3.407802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.107200</td>\n",
       "      <td>3.211393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.604200</td>\n",
       "      <td>2.929364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.403900</td>\n",
       "      <td>2.621439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.770100</td>\n",
       "      <td>2.392522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.509800</td>\n",
       "      <td>2.262323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.366500</td>\n",
       "      <td>2.126256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.039100</td>\n",
       "      <td>1.964806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.707600</td>\n",
       "      <td>1.780536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.607600</td>\n",
       "      <td>1.603363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.253300</td>\n",
       "      <td>1.461827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.242300</td>\n",
       "      <td>1.340434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.005400</td>\n",
       "      <td>1.237364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.795700</td>\n",
       "      <td>1.145180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.803100</td>\n",
       "      <td>1.084830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.584100</td>\n",
       "      <td>1.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.527600</td>\n",
       "      <td>0.997438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.433200</td>\n",
       "      <td>0.974541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.457700</td>\n",
       "      <td>0.959652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.437000</td>\n",
       "      <td>0.947605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.369800</td>\n",
       "      <td>0.938684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.378000</td>\n",
       "      <td>0.934287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.332500</td>\n",
       "      <td>0.924429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.262200</td>\n",
       "      <td>0.916049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.250400</td>\n",
       "      <td>0.906897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.292000</td>\n",
       "      <td>0.900851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.238000</td>\n",
       "      <td>0.897186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.197800</td>\n",
       "      <td>0.897850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.108800</td>\n",
       "      <td>0.897998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.172300</td>\n",
       "      <td>0.886737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.127500</td>\n",
       "      <td>0.882408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.146600</td>\n",
       "      <td>0.879887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.146800</td>\n",
       "      <td>0.876650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.147400</td>\n",
       "      <td>0.875472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.103400</td>\n",
       "      <td>0.868898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.209300</td>\n",
       "      <td>0.867909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.134400</td>\n",
       "      <td>0.863213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.162900</td>\n",
       "      <td>0.859904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.082100</td>\n",
       "      <td>0.856464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.047200</td>\n",
       "      <td>0.860150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.112000</td>\n",
       "      <td>0.855005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.100300</td>\n",
       "      <td>0.853913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.092600</td>\n",
       "      <td>0.847143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.040700</td>\n",
       "      <td>0.847894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.023200</td>\n",
       "      <td>0.860268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.038900</td>\n",
       "      <td>0.845217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.035700</td>\n",
       "      <td>0.847598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.017100</td>\n",
       "      <td>0.845026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.044300</td>\n",
       "      <td>0.838555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.984500</td>\n",
       "      <td>0.841117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.004100</td>\n",
       "      <td>0.840953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.034100</td>\n",
       "      <td>0.835177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.967800</td>\n",
       "      <td>0.834567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.977300</td>\n",
       "      <td>0.837816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>0.828756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.012200</td>\n",
       "      <td>0.833984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.954300</td>\n",
       "      <td>0.836783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>0.829520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.007700</td>\n",
       "      <td>0.830502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>0.831726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=1.8572383350796169, metrics={'train_runtime': 1062.3939, 'train_samples_per_second': 37.651, 'train_steps_per_second': 4.706, 'total_flos': 3.2329168415760384e+16, 'train_loss': 1.8572383350796169, 'epoch': 4.846153846153846})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.arange(len(train_inputs))\n",
    "dataset = create_dataset(idxs, train_inputs,tokenizer)\n",
    "vidxs = np.arange(len(val_inputs))\n",
    "valdataset = create_dataset(vidxs, val_inputs,tokenizer)\n",
    "\n",
    "# Define EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Number of evaluations with no improvement before stopping\n",
    "    early_stopping_threshold=0.0  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=args.max_seq_length,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=args.micro_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        max_steps=args.max_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        weight_decay=args.weight_decay,\n",
    "        fp16=args.fp16,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        seed=args.seed,\n",
    "        run_name=f\"train-{args.model_id.split('/')[-1]}-tfix\",\n",
    "        report_to=\"wandb\",\n",
    "        load_best_model_at_end=True,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=10,\n",
    "        save_steps=10,\n",
    "        save_strategy='steps',\n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        save_total_limit = 3,\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    eval_dataset = valdataset,\n",
    "    callbacks=[early_stopping]     \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875edc96-63e2-4c97-b981-62c97f8c4764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7751b9e-eb5d-4f68-b26a-7d33376c9f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e0eefe-9897-4d53-a04a-97b15a36eff1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f841f-5db8-48fc-8de4-6e141288e195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058858aa80834d63beb06306cfaa5f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationMode.GREEDY_SEARCH\n",
      "Current Score: 12 / 32\n",
      "GenerationMode.GREEDY_SEARCH\n",
      "Current Score: 26 / 64\n",
      "GenerationMode.GREEDY_SEARCH\n",
      "Current Score: 40 / 96\n",
      "GenerationMode.GREEDY_SEARCH\n",
      "Current Score: 51 / 128\n",
      "GenerationMode.GREEDY_SEARCH\n",
      "Current Score: 51 / 129\n",
      "Exact Match (EM) Score: 0.3953488372093023\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch_size = 32 \n",
    "# Faster than tokenizing every time\n",
    "all_tokenized_inputs = tokenizer(test_inputs, return_tensors=\"pt\", truncation=True, max_length=512-2, padding=True)\n",
    "all_tokenized_inputs = {k: v.to('cuda') for k, v in all_tokenized_inputs.items()}\n",
    "\n",
    "# Bar to check progress\n",
    "pbar = tqdm(range(0, len(test_data['fixed']), batch_size))\n",
    "ems = []\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for i in range(0, len(test_data['fixed']), batch_size):\n",
    "        # Prepare the batch\n",
    "        batch_inputs = {k: v[i:i + batch_size] for k, v in all_tokenized_inputs.items()}\n",
    "        batch_answers = test_data['fixed'][i:i + batch_size]\n",
    "\n",
    "        # Mixed precision inference for speedup\n",
    "        with autocast():\n",
    "            outputs = model.generate(input_ids=batch_inputs['input_ids'], \n",
    "                                     attention_mask=batch_inputs['attention_mask'],\n",
    "                                     num_beams=1, max_length=512, # Greedy decoding\n",
    "                                     pad_token_id=tokenizer.eos_token_id, \n",
    "                                     early_stopping=True)\n",
    "\n",
    "        # Decode the outputs\n",
    "        batch_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        # Compare predictions with answers\n",
    "        for j, (tout, tanswer) in enumerate(zip(batch_outputs, batch_answers)):\n",
    "            tout = tout.split('Fixed:')\n",
    "            if len(tout) < 2:\n",
    "                fixed = tout[0].strip()\n",
    "            else:\n",
    "                fixed = tout[1].strip()\n",
    "\n",
    "            # Check exact match with developer fix\n",
    "            fixed = ''.join([x.strip() for x in fixed.split()])\n",
    "            tanswer = tanswer.strip()\n",
    "            tanswer = ''.join([x.strip() for x in tanswer.split()]).replace('<|endoftext|>', '')\n",
    "            em = 1 if fixed == tanswer else 0\n",
    "            ems.append(em)\n",
    "\n",
    "        count += batch_size\n",
    "        print(f\"Current Score: {sum(ems)} / {len(ems)}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "# Calculate overall exact match (EM) score\n",
    "em_score = sum(ems) / len(ems)\n",
    "print(f\"Exact Match (EM) Score: {em_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d6219-220f-4722-ac9a-d5ff5ea79516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
